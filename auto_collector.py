#!/usr/bin/env python3
"""
ìë™ ë…¼ë¬¸ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ë§¤ì¼ ì§€ì •ëœ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ìë™ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import requests
import json
import time
import schedule
from datetime import datetime, timedelta
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_collector.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class AutoPaperCollector:
    def __init__(self, server_url="http://localhost:8001"):
        self.server_url = server_url
        self.session = requests.Session()
        
        # ìˆ˜ì§‘í•  í‚¤ì›Œë“œì™€ ì„¤ì • (ë‹¤ì–‘í•œ í•™ë¬¸ ë¶„ì•¼)
        self.keywords = [
            # ğŸ§  ì¸ê³µì§€ëŠ¥/ì»´í“¨í„° ê³¼í•™
            "machine learning",
            "deep learning", 
            "artificial intelligence",
            "neural networks",
            "computer vision",
            "natural language processing",
            "large language models",
            "transformer models",
            
            # ğŸ§¬ ìƒëª…ê³¼í•™/ì˜í•™
            "cancer research",
            "drug discovery",
            "genomics",
            "proteomics",
            "immunology",
            "neuroscience",
            "microbiology",
            "biotechnology",
            "medical imaging",
            "clinical trials",
            "epidemiology",
            "pharmacology",
            
            # ğŸ¾ ë™ë¬¼í•™/ìƒíƒœí•™
            "animal behavior",
            "wildlife conservation",
            "marine biology",
            "evolutionary biology",
            "ecology",
            "biodiversity",
            "zoology",
            "ornithology",
            
            # ğŸŒ ì§€êµ¬ê³¼í•™/í™˜ê²½
            "geology",
            "climate change",
            "atmospheric science",
            "oceanography",
            "seismology",
            "volcanology",
            "environmental science",
            "sustainability",
            "renewable energy",
            
            # ğŸ”¬ ë¬¼ë¦¬í•™/í™”í•™
            "quantum physics",
            "particle physics",
            "astrophysics",
            "condensed matter physics",
            "nuclear physics",
            "physical chemistry",
            "organic chemistry",
            "inorganic chemistry",
            "biochemistry",
            "materials science",
            
            # ğŸ“ ìˆ˜í•™
            "mathematical analysis",
            "algebra",
            "geometry",
            "topology",
            "number theory",
            "statistics",
            "probability theory",
            "optimization",
            "differential equations",
            
            # âš¡ ì „ê¸°/ì „ìê³µí•™
            "electrical engineering",
            "electronics",
            "semiconductor physics",
            "power systems",
            "control systems",
            "signal processing",
            "telecommunications",
            "robotics",
            
            # ğŸ¨ ì˜ˆìˆ /ë””ìì¸
            "digital art",
            "computer graphics",
            "human-computer interaction",
            "user interface design",
            "creative computing",
            "generative art",
            "virtual reality",
            "augmented reality",
            
            # ğŸ—ï¸ ê±´ì¶•/í† ëª©
            "architecture",
            "structural engineering",
            "civil engineering",
            "construction materials",
            "urban planning",
            "transportation engineering",
            "environmental engineering",
            
            # ğŸ’° ê²½ì œ/ê¸ˆìœµ
            "financial economics",
            "behavioral economics",
            "financial modeling",
            "risk management",
            "investment analysis",
            "market research",
            "economic forecasting",
            
            # ğŸ§  ì‹¬ë¦¬í•™/ì‚¬íšŒí•™
            "cognitive psychology",
            "social psychology",
            "developmental psychology",
            "clinical psychology",
            "sociology",
            "anthropology",
            "political science",
            
            # ğŸ“š êµìœ¡/ì–¸ì–´í•™
            "educational technology",
            "language learning",
            "linguistics",
            "cognitive science",
            "pedagogy",
            "curriculum design",
            
            # ğŸ¥ ì˜ë£Œê¸°ê¸°/ë°”ì´ì˜¤ë©”ë””ì»¬
            "biomedical engineering",
            "medical devices",
            "tissue engineering",
            "biomechanics",
            "bioinformatics",
            "precision medicine",
            
            # ğŸŒ± ë†ì—…/ì‹í’ˆê³¼í•™
            "agricultural science",
            "food science",
            "plant biology",
            "soil science",
            "agricultural technology",
            "food safety",
            
            # ğŸ”’ ë³´ì•ˆ/ì‚¬ì´ë²„ë³´ì•ˆ
            "cybersecurity",
            "cryptography",
            "network security",
            "information security",
            "privacy protection",
            "digital forensics"
        ]
        
        # ê° í‚¤ì›Œë“œë³„ ì„¤ì • (ê· í˜•ì¡íŒ ìˆ˜ì§‘)
        self.keyword_configs = {
            # ğŸ§  ì¸ê³µì§€ëŠ¥/ì»´í“¨í„° ê³¼í•™ (ì¤‘ê°„ ìˆ˜ì§‘)
            "machine learning": {"max_results": 10, "source": "arxiv"},
            "deep learning": {"max_results": 10, "source": "arxiv"},
            "artificial intelligence": {"max_results": 8, "source": "all"},
            "neural networks": {"max_results": 8, "source": "arxiv"},
            "computer vision": {"max_results": 8, "source": "arxiv"},
            "natural language processing": {"max_results": 8, "source": "arxiv"},
            "large language models": {"max_results": 8, "source": "arxiv"},
            "transformer models": {"max_results": 6, "source": "arxiv"},
            
            # ğŸ§¬ ìƒëª…ê³¼í•™/ì˜í•™ (ë§ì´ ìˆ˜ì§‘)
            "cancer research": {"max_results": 12, "source": "all"},
            "drug discovery": {"max_results": 10, "source": "all"},
            "genomics": {"max_results": 10, "source": "all"},
            "proteomics": {"max_results": 8, "source": "all"},
            "immunology": {"max_results": 8, "source": "all"},
            "neuroscience": {"max_results": 10, "source": "all"},
            "microbiology": {"max_results": 8, "source": "all"},
            "biotechnology": {"max_results": 8, "source": "all"},
            "medical imaging": {"max_results": 8, "source": "all"},
            "clinical trials": {"max_results": 6, "source": "all"},
            "epidemiology": {"max_results": 8, "source": "all"},
            "pharmacology": {"max_results": 8, "source": "all"},
            
            # ğŸ¾ ë™ë¬¼í•™/ìƒíƒœí•™ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "animal behavior": {"max_results": 6, "source": "all"},
            "wildlife conservation": {"max_results": 6, "source": "all"},
            "marine biology": {"max_results": 6, "source": "all"},
            "evolutionary biology": {"max_results": 6, "source": "all"},
            "ecology": {"max_results": 6, "source": "all"},
            "biodiversity": {"max_results": 6, "source": "all"},
            "zoology": {"max_results": 5, "source": "all"},
            "ornithology": {"max_results": 5, "source": "all"},
            
            # ğŸŒ ì§€êµ¬ê³¼í•™/í™˜ê²½ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "geology": {"max_results": 8, "source": "all"},
            "climate change": {"max_results": 8, "source": "all"},
            "atmospheric science": {"max_results": 6, "source": "all"},
            "oceanography": {"max_results": 6, "source": "all"},
            "seismology": {"max_results": 5, "source": "all"},
            "volcanology": {"max_results": 5, "source": "all"},
            "environmental science": {"max_results": 8, "source": "all"},
            "sustainability": {"max_results": 6, "source": "all"},
            "renewable energy": {"max_results": 6, "source": "all"},
            
            # ğŸ”¬ ë¬¼ë¦¬í•™/í™”í•™ (ë§ì´ ìˆ˜ì§‘)
            "quantum physics": {"max_results": 10, "source": "arxiv"},
            "particle physics": {"max_results": 8, "source": "arxiv"},
            "astrophysics": {"max_results": 8, "source": "arxiv"},
            "condensed matter physics": {"max_results": 8, "source": "arxiv"},
            "nuclear physics": {"max_results": 6, "source": "arxiv"},
            "physical chemistry": {"max_results": 8, "source": "all"},
            "organic chemistry": {"max_results": 8, "source": "all"},
            "inorganic chemistry": {"max_results": 6, "source": "all"},
            "biochemistry": {"max_results": 8, "source": "all"},
            "materials science": {"max_results": 8, "source": "all"},
            
            # ğŸ“ ìˆ˜í•™ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "mathematical analysis": {"max_results": 8, "source": "arxiv"},
            "algebra": {"max_results": 6, "source": "arxiv"},
            "geometry": {"max_results": 6, "source": "arxiv"},
            "topology": {"max_results": 6, "source": "arxiv"},
            "number theory": {"max_results": 6, "source": "arxiv"},
            "statistics": {"max_results": 8, "source": "arxiv"},
            "probability theory": {"max_results": 6, "source": "arxiv"},
            "optimization": {"max_results": 6, "source": "arxiv"},
            "differential equations": {"max_results": 6, "source": "arxiv"},
            
            # âš¡ ì „ê¸°/ì „ìê³µí•™ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "electrical engineering": {"max_results": 8, "source": "arxiv"},
            "electronics": {"max_results": 8, "source": "arxiv"},
            "semiconductor physics": {"max_results": 6, "source": "arxiv"},
            "power systems": {"max_results": 6, "source": "arxiv"},
            "control systems": {"max_results": 6, "source": "arxiv"},
            "signal processing": {"max_results": 6, "source": "arxiv"},
            "telecommunications": {"max_results": 6, "source": "arxiv"},
            "robotics": {"max_results": 8, "source": "arxiv"},
            
            # ğŸ¨ ì˜ˆìˆ /ë””ìì¸ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "digital art": {"max_results": 5, "source": "arxiv"},
            "computer graphics": {"max_results": 6, "source": "arxiv"},
            "human-computer interaction": {"max_results": 6, "source": "arxiv"},
            "user interface design": {"max_results": 5, "source": "arxiv"},
            "creative computing": {"max_results": 5, "source": "arxiv"},
            "generative art": {"max_results": 5, "source": "arxiv"},
            "virtual reality": {"max_results": 6, "source": "arxiv"},
            "augmented reality": {"max_results": 6, "source": "arxiv"},
            
            # ğŸ—ï¸ ê±´ì¶•/í† ëª© (ì ë‹¹íˆ ìˆ˜ì§‘)
            "architecture": {"max_results": 6, "source": "arxiv"},
            "structural engineering": {"max_results": 6, "source": "arxiv"},
            "civil engineering": {"max_results": 6, "source": "arxiv"},
            "construction materials": {"max_results": 5, "source": "arxiv"},
            "urban planning": {"max_results": 5, "source": "arxiv"},
            "transportation engineering": {"max_results": 5, "source": "arxiv"},
            "environmental engineering": {"max_results": 6, "source": "arxiv"},
            
            # ğŸ’° ê²½ì œ/ê¸ˆìœµ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "financial economics": {"max_results": 6, "source": "arxiv"},
            "behavioral economics": {"max_results": 6, "source": "arxiv"},
            "financial modeling": {"max_results": 6, "source": "arxiv"},
            "risk management": {"max_results": 5, "source": "arxiv"},
            "investment analysis": {"max_results": 5, "source": "arxiv"},
            "market research": {"max_results": 5, "source": "arxiv"},
            "economic forecasting": {"max_results": 5, "source": "arxiv"},
            
            # ğŸ§  ì‹¬ë¦¬í•™/ì‚¬íšŒí•™ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "cognitive psychology": {"max_results": 6, "source": "arxiv"},
            "social psychology": {"max_results": 6, "source": "arxiv"},
            "developmental psychology": {"max_results": 5, "source": "arxiv"},
            "clinical psychology": {"max_results": 6, "source": "arxiv"},
            "sociology": {"max_results": 6, "source": "arxiv"},
            "anthropology": {"max_results": 5, "source": "arxiv"},
            "political science": {"max_results": 5, "source": "arxiv"},
            
            # ğŸ“š êµìœ¡/ì–¸ì–´í•™ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "educational technology": {"max_results": 6, "source": "arxiv"},
            "language learning": {"max_results": 5, "source": "arxiv"},
            "linguistics": {"max_results": 6, "source": "arxiv"},
            "cognitive science": {"max_results": 6, "source": "arxiv"},
            "pedagogy": {"max_results": 5, "source": "arxiv"},
            "curriculum design": {"max_results": 5, "source": "arxiv"},
            
            # ğŸ¥ ì˜ë£Œê¸°ê¸°/ë°”ì´ì˜¤ë©”ë””ì»¬ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "biomedical engineering": {"max_results": 8, "source": "arxiv"},
            "medical devices": {"max_results": 6, "source": "arxiv"},
            "tissue engineering": {"max_results": 6, "source": "arxiv"},
            "biomechanics": {"max_results": 6, "source": "arxiv"},
            "bioinformatics": {"max_results": 8, "source": "arxiv"},
            "precision medicine": {"max_results": 6, "source": "arxiv"},
            
            # ğŸŒ± ë†ì—…/ì‹í’ˆê³¼í•™ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "agricultural science": {"max_results": 6, "source": "arxiv"},
            "food science": {"max_results": 6, "source": "arxiv"},
            "plant biology": {"max_results": 6, "source": "arxiv"},
            "soil science": {"max_results": 5, "source": "arxiv"},
            "agricultural technology": {"max_results": 5, "source": "arxiv"},
            "food safety": {"max_results": 5, "source": "arxiv"},
            
            # ğŸ”’ ë³´ì•ˆ/ì‚¬ì´ë²„ë³´ì•ˆ (ì ë‹¹íˆ ìˆ˜ì§‘)
            "cybersecurity": {"max_results": 6, "source": "arxiv"},
            "cryptography": {"max_results": 6, "source": "arxiv"},
            "network security": {"max_results": 5, "source": "arxiv"},
            "information security": {"max_results": 5, "source": "arxiv"},
            "privacy protection": {"max_results": 5, "source": "arxiv"},
            "digital forensics": {"max_results": 5, "source": "arxiv"}
        }
    
    def collect_papers_for_keyword(self, keyword, time_folder=None):
        """íŠ¹ì • í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ìˆ˜ì§‘ ë° PDF ë‹¤ìš´ë¡œë“œ"""
        try:
            config = self.keyword_configs.get(keyword, {"max_results": 5, "source": "arxiv"})
            
            logging.info(f"'{keyword}' í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘...")
            
            response = self.session.post(f"{self.server_url}/search_papers", json={
                "query": keyword,
                "max_results": config["max_results"],
                "source": config["source"]
            }, timeout=60)
            
            if response.status_code == 200:
                papers = response.json()
                logging.info(f"'{keyword}': {len(papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
                
                # ìˆ˜ì§‘ëœ ë…¼ë¬¸ ì •ë³´ ë¡œê¹… ë° PDF ë‹¤ìš´ë¡œë“œ
                downloaded_count = 0
                skipped_long_papers = 0
                
                for i, paper in enumerate(papers, 1):
                    logging.info(f"  {i}. {paper['title']} ({paper['source']})")
                    
                    # PDF ë‹¤ìš´ë¡œë“œ ì‹œë„
                    if paper.get('pdf_url'):
                        try:
                            download_response = self.session.post(f"{self.server_url}/download_paper", 
                                                                params={"paper_url": paper['url'], "time_folder": time_folder}, timeout=60)
                            
                            if download_response.status_code == 200:
                                result = download_response.json()
                                if result.get('success'):
                                    logging.info(f"    ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {result.get('filename', 'N/A')}")
                                    downloaded_count += 1
                                else:
                                    error_msg = result.get('error', 'Unknown error')
                                    if 'too long' in error_msg.lower():
                                        logging.info(f"    â­ï¸ í˜ì´ì§€ ìˆ˜ ì´ˆê³¼ë¡œ ê±´ë„ˆëœ€: {error_msg}")
                                        skipped_long_papers += 1
                                    else:
                                        logging.warning(f"    âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
                            else:
                                logging.warning(f"    âŒ PDF ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì‹¤íŒ¨: {download_response.status_code}")
                                
                        except Exception as e:
                            logging.warning(f"    âŒ PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
                    
                    # ë‹¤ìš´ë¡œë“œ ê°„ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    time.sleep(1.5)
                
                logging.info(f"'{keyword}': {downloaded_count}ê°œ PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, {skipped_long_papers}ê°œ í˜ì´ì§€ ìˆ˜ ì´ˆê³¼ë¡œ ê±´ë„ˆëœ€")
                return papers
            else:
                logging.error(f"'{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            logging.error(f"'{keyword}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def daily_collection(self):
        """ë§¤ì¼ ì‹¤í–‰í•  ë…¼ë¬¸ ìˆ˜ì§‘ ì‘ì—…"""
        # ì‹¤í–‰ ì‹œê°„ ê¸°ì¤€ í´ë” ìƒì„±
        current_time = datetime.now()
        time_folder = current_time.strftime('%Y%m%d_%H%M%S')
        
        logging.info("=" * 60)
        logging.info(f"ì¼ì¼ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘ - {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logging.info(f"ğŸ“ ì €ì¥ í´ë”: {time_folder}")
        logging.info("=" * 60)
        
        # ì‹œê°„ë³„ í´ë” ìƒì„±
        import os
        from pathlib import Path
        
        papers_dir = Path("papers")
        time_papers_dir = papers_dir / time_folder
        time_papers_dir.mkdir(parents=True, exist_ok=True)
        
        logging.info(f"ğŸ“‚ í´ë” ìƒì„± ì™„ë£Œ: {time_papers_dir}")
        
        total_collected = 0
        total_downloaded = 0
        total_skipped = 0
        
        for i, keyword in enumerate(self.keywords, 1):
            logging.info(f"ì§„í–‰ë¥ : {i}/{len(self.keywords)} - '{keyword}' ì²˜ë¦¬ ì¤‘...")
            papers = self.collect_papers_for_keyword(keyword, time_folder)
            total_collected += len(papers)
            
            # í‚¤ì›Œë“œ ê°„ ì§€ì—° ì‹œê°„ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(self.keywords):  # ë§ˆì§€ë§‰ í‚¤ì›Œë“œê°€ ì•„ë‹ˆë©´ ì§€ì—°
                time.sleep(3)
        
        logging.info("=" * 60)
        logging.info(f"ì¼ì¼ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {total_collected}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")
        logging.info(f"í‚¤ì›Œë“œ ìˆ˜: {len(self.keywords)}ê°œ")
        logging.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {time_papers_dir}")
        logging.info("=" * 60)
    
    def weekly_summary(self):
        """ì£¼ê°„ ìš”ì•½ ìƒì„±"""
        try:
            logging.info("=" * 60)
            logging.info(f"ì£¼ê°„ ìš”ì•½ ìƒì„± ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logging.info("=" * 60)
            
            response = self.session.get(f"{self.server_url}/papers?limit=200")
            
            if response.status_code == 200:
                papers = response.json()
                
                # ìµœê·¼ 7ì¼ê°„ ìˆ˜ì§‘ëœ ë…¼ë¬¸ í•„í„°ë§
                week_ago = datetime.now() - timedelta(days=7)
                
                recent_papers = []
                for paper in papers:
                    try:
                        # ë‚ ì§œ íŒŒì‹± ê°œì„ 
                        created_at = paper['created_at']
                        logging.info(f"íŒŒì‹± ì¤‘ì¸ ë‚ ì§œ: {created_at}")
                        
                        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                        if 'T' in created_at:
                            # ISO í˜•ì‹: 2025-07-23T08:15:12
                            paper_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        elif ' ' in created_at:
                            # SQL í˜•ì‹: 2025-07-23 08:15:12
                            paper_date = datetime.strptime(created_at, '%Y-%m-%d %H:%M:%S')
                        else:
                            # ë‚ ì§œë§Œ: 2025-07-23
                            paper_date = datetime.strptime(created_at, '%Y-%m-%d')
                        
                        logging.info(f"íŒŒì‹±ëœ ë‚ ì§œ: {paper_date}, ê¸°ì¤€ì¼: {week_ago}")
                        
                        if paper_date > week_ago:
                            recent_papers.append(paper)
                            logging.info(f"ìµœê·¼ ë…¼ë¬¸ ì¶”ê°€: {paper['title'][:50]}...")
                    except Exception as e:
                        logging.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {paper.get('created_at', 'N/A')} - {e}")
                        continue
                
                # 1. ê¸°ë³¸ í†µê³„
                logging.info(f"ğŸ“Š ì£¼ê°„ ìš”ì•½: ìµœê·¼ 7ì¼ê°„ {len(recent_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")
                
                # 2. ì†ŒìŠ¤ë³„ í†µê³„
                source_stats = {}
                for paper in recent_papers:
                    source = paper['source']
                    source_stats[source] = source_stats.get(source, 0) + 1
                
                logging.info("ğŸ“ˆ ì†ŒìŠ¤ë³„ í†µê³„:")
                for source, count in source_stats.items():
                    logging.info(f"  ğŸ“š {source}: {count}ê°œ")
                
                # 3. í‚¤ì›Œë“œë³„ í†µê³„ (ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­)
                keyword_stats = {}
                for paper in recent_papers:
                    title_lower = paper['title'].lower()
                    for keyword in self.keywords:
                        if keyword.lower() in title_lower:
                            keyword_stats[keyword] = keyword_stats.get(keyword, 0) + 1
                
                logging.info("ğŸ” í‚¤ì›Œë“œë³„ í†µê³„:")
                for keyword, count in keyword_stats.items():
                    logging.info(f"  ğŸ·ï¸ {keyword}: {count}ê°œ")
                
                # 4. PDF ë‹¤ìš´ë¡œë“œ í†µê³„
                pdf_downloaded = sum(1 for paper in recent_papers if paper.get('file_path'))
                pdf_total = len(recent_papers)
                pdf_rate = (pdf_downloaded / pdf_total * 100) if pdf_total > 0 else 0
                
                logging.info(f"ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ: {pdf_downloaded}/{pdf_total}ê°œ ({pdf_rate:.1f}%)")
                
                # 5. ìµœê·¼ ë…¼ë¬¸ ëª©ë¡ (ìƒìœ„ 10ê°œ)
                logging.info("ğŸ“‹ ìµœê·¼ ìˆ˜ì§‘ëœ ë…¼ë¬¸ (ìƒìœ„ 10ê°œ):")
                for i, paper in enumerate(recent_papers[:10], 1):
                    pdf_status = "âœ…" if paper.get('file_path') else "âŒ"
                    logging.info(f"  {i}. {pdf_status} {paper['title']}")
                    logging.info(f"     ğŸ‘¥ ì €ì: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                    logging.info(f"     ğŸ“… ìˆ˜ì§‘ì¼: {paper['created_at'][:10]}")
                    logging.info(f"     ğŸ”— ì†ŒìŠ¤: {paper['source']}")
                    logging.info("")
                
                # 6. ì¼ë³„ ìˆ˜ì§‘ í†µê³„
                daily_stats = {}
                for paper in recent_papers:
                    date = paper['created_at'][:10]  # YYYY-MM-DD
                    daily_stats[date] = daily_stats.get(date, 0) + 1
                
                logging.info("ğŸ“… ì¼ë³„ ìˆ˜ì§‘ í†µê³„:")
                for date in sorted(daily_stats.keys()):
                    count = daily_stats[date]
                    logging.info(f"  ğŸ“† {date}: {count}ê°œ")
                
                # 7. ì¶”ì²œ í‚¤ì›Œë“œ (ìˆ˜ì§‘ì´ ì ì€ í‚¤ì›Œë“œ)
                logging.info("ğŸ’¡ ì¶”ì²œ ì‚¬í•­:")
                for keyword in self.keywords:
                    if keyword not in keyword_stats or keyword_stats[keyword] < 2:
                        logging.info(f"  ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ë” ë§ì€ ë…¼ë¬¸ ìˆ˜ì§‘ ê¶Œì¥")
                
                logging.info("=" * 60)
                logging.info("ì£¼ê°„ ìš”ì•½ ì™„ë£Œ")
                logging.info("=" * 60)
                
            else:
                logging.error(f"ì£¼ê°„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logging.error(f"ì£¼ê°„ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def run_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
        logging.info("ìë™ ë…¼ë¬¸ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
        
        # ë§¤ì¼ ì˜¤ì „ 9ì‹œì— ìˆ˜ì§‘
        schedule.every().day.at("09:00").do(self.daily_collection)
        
        # ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 10ì‹œì— ì£¼ê°„ ìš”ì•½
        schedule.every().sunday.at("10:00").do(self.weekly_summary)
        
        # ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
        logging.info("ì¦‰ì‹œ ì²« ë²ˆì§¸ ìˆ˜ì§‘ ì‹¤í–‰...")
        self.daily_collection()
        
        logging.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
        except KeyboardInterrupt:
            logging.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    collector = AutoPaperCollector()
    
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == "collect":
            # ì¦‰ì‹œ ìˆ˜ì§‘ë§Œ ì‹¤í–‰
            collector.daily_collection()
        elif sys.argv[1] == "summary":
            # ì£¼ê°„ ìš”ì•½ë§Œ ì‹¤í–‰
            collector.weekly_summary()
        else:
            print("ì‚¬ìš©ë²•: python auto_collector.py [collect|summary]")
    else:
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
        collector.run_scheduler()

if __name__ == "__main__":
    main() 